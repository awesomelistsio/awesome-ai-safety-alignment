# Awesome AI Safety & Alignment [![Awesome Lists](https://srv-cdn.himpfen.io/badges/awesome-lists/awesomelists-flat.svg)](https://github.com/awesomelistsio/awesome)

[![Ko-Fi](https://srv-cdn.himpfen.io/badges/kofi/kofi-flat.svg)](https://ko-fi.com/awesomelists) &nbsp; [![PayPal](https://srv-cdn.himpfen.io/badges/paypal/paypal-flat.svg)](https://www.paypal.com/donate/?hosted_button_id=3LLKRXJU44EJJ) &nbsp; [![Stripe](https://srv-cdn.himpfen.io/badges/stripe/stripe-flat.svg)](https://tinyurl.com/e8ymxdw3) &nbsp; [![X](https://srv-cdn.himpfen.io/badges/twitter/twitter-flat.svg)](https://x.com/ListsAwesome) &nbsp; [![Facebook](https://srv-cdn.himpfen.io/badges/facebook-pages/facebook-pages-flat.svg)](https://www.facebook.com/awesomelists)

> A curated list of research, frameworks, tools, evaluations, and resources focused on AI alignment, safety, robustness, red-teaming, model governance, and responsible development.

## Contents

- [Research Organizations](#research-organizations)
- [Safety Frameworks](#safety-frameworks)
- [Red Teaming & Threat Modeling](#red-teaming--threat-modeling)
- [Evaluation & Benchmarks](#evaluation--benchmarks)
- [Model Governance & Policy](#model-governance--policy)
- [Datasets](#datasets)
- [Learning Resources](#learning-resources)
- [Related Awesome Lists](#related-awesome-lists)

## Research Organizations

- [Alignment Research Center (ARC)](https://alignment.org/) – Research on scalable oversight and model evaluations.
- [AI Safety Center (UK)](https://www.aisc.gov.uk/) – Government-backed safety and model evaluation initiatives.
- [OpenAI Safety](https://openai.com/safety) – Research on robustness, red-teaming, and alignment.
- [Anthropic Safety](https://www.anthropic.com/safety) – Safety teams working on interpretability and frontier model evaluations.
- [DeepMind Safety Research](https://deepmind.google/discover/blog) – Research into scalable oversight, alignment, robustness.
- [Center for AI Safety (CAIS)](https://www.safe.ai/) – Public safety education, benchmarks, and policy guidance.
- [ELEUTHERAI](https://www.eleuther.ai/) – Open-source AI research with safety-driven initiatives.
  
## Safety Frameworks

- [OpenAI Model Spec](https://openai.com/model-spec) – Specifications defining expected safe model behavior.
- [Anthropic Constitutional AI](https://www.anthropic.com/news/constitutional-ai) – Framework for training models using rule-based constitutional constraints.
- [Google Responsible AI Practices](https://ai.google/responsibility/) – Principles and frameworks for safe AI development.
- [OECD AI Principles](https://oecd.ai/en/ai-principles) – International standards for trustworthy AI.
- [NIST AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework) – U.S. national standard for assessing AI risk.
- [EU AI Act Summary](https://artificialintelligenceact.eu/) – Regulatory framework for high-risk and general-purpose AI systems.

## Red Teaming & Threat Modeling

- [OpenAI Red Teaming Network](https://openai.com/red-teaming-network) – Global research collaboration for model evaluations.
- [Anthropic Red Teaming Resources](https://www.anthropic.com/) – Safety-focused adversarial testing methods.
- [Microsoft AI Red Team](https://www.microsoft.com/en-us/security/blog/) – Methodologies for security and safety testing of AI systems.
- [AI Safety Threat Modeling](https://github.com/topics/ai-safety) – Community tools and docs for threat analysis.
- [LLM Jailbreak Prompts Datasets](https://github.com/topics/jailbreak-prompts) – Collections of adversarial prompts for robustness testing.

## Evaluation & Benchmarks

- [HELM](https://crfm.stanford.edu/helm/latest/) – Holistic evaluation of language models across safety and risk domains.
- [Anthropic Evaluations](https://github.com/anthropics/evals) – Safety evaluations for frontier models.
- [OpenAI Evals](https://github.com/openai/evals) – Framework for testing model safety, reasoning, and reliability.
- [Red Teaming Benchmarks](https://github.com/topics/llm-evaluation) – Community-driven safety evaluations.
- [ToxiGen](https://github.com/microsoft/Counterfit/) – Datasets for evaluating harmful or toxic outputs.
- [SafetyBench](https://github.com/centerforaisafety/SafetyBench) – Benchmark framework for AI safety scenarios.

## Model Governance & Policy

- [AI Safety Institute (UK)](https://www.aisi.gov.uk/) – International coordination on frontier model safety testing.
- [AI Safety Institute (US)](https://www.ai.gov/) – U.S. policy, evaluations, and governance efforts.
- [OECD AI Governance Hub](https://oecd.ai/en/) – Regulatory and policy resources for AI alignment.
- [UNESCO AI Ethics Framework](https://www.unesco.org/en/artificial-intelligence/ethics) – Global normative framework for ethical AI.
- [Global AI Safety Summits](https://www.gov.uk/government/publications) – Agreements and charters from global model safety gatherings.

## Datasets

- [JailbreakBench](https://github.com/verazuo/jailbreakbench) – Evaluation dataset for jailbreak susceptibility.
- [HarmBench](https://github.com/centerforaisafety/HarmBench) – Multi-domain dataset for AI harm classification and safety testing.
- [RealToxicityPrompts](https://allenai.org/data/real-toxicity-prompts) – Adversarial or harmful prompts used in robustness evaluation.
- [AdvBench](https://github.com/safety-ai/AdvBench) – Dataset for adversarial attacks and safety testing.

## Learning Resources

- [AI Alignment Fundamentals (BlueDot)](https://www.alignmentfundamentals.com/) – Intro curriculum for alignment.
- [AGI Safety Fundamentals](https://agi-safety-fundamentals.com/) – Structured course on alignment, safety, and governance.
- [OpenAI Safety Papers](https://openai.com/research) – Research papers on alignment and model evaluations.
- [Anthropic Interpretability Research](https://www.anthropic.com/research) – Papers and findings on model internals.
- [DeepMind Safety Papers](https://deepmind.google/research) – Research into oversight, robustness, and alignment.
- [CAIS Safety Curriculum](https://www.safe.ai/) – Intro and advanced learning pathways.

## Related Awesome Lists

- [Awesome AI](https://github.com/awesomelistsio/awesome-ai)
- [Awesome Machine Learning](https://github.com/awesomelistsio/awesome-machine-learning)
- [Awesome AI Research Papers](https://github.com/awesomelistsio/awesome-ai-research-papers)
- [Awesome AI Ethics](https://github.com/awesomelistsio/awesome-ai-ethics)
- [Awesome Open Governance](https://github.com/awesomelistsio/awesome-open-governance)

## Contribute

Contributions are welcome!

## License

[![CC0](https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/by-sa.svg)](http://creativecommons.org/licenses/by-sa/4.0/)
